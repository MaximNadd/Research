{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11228747,"sourceType":"datasetVersion","datasetId":7013542}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":30.776915,"end_time":"2025-03-21T01:39:31.730854","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-21T01:39:00.953939","version":"2.6.0"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nibabel as nib\nimport random\nfrom scipy import ndimage\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, Conv3D, MaxPool3D, GlobalAveragePooling3D, Dense, Dropout, BatchNormalization, concatenate\nfrom tensorflow.keras.models import Model, load_model\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.utils import plot_model\nfrom keras.metrics import AUC, F1Score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import mixed_precision\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import Sequence","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-04-06T13:31:36.236487Z","iopub.execute_input":"2025-04-06T13:31:36.236791Z","iopub.status.idle":"2025-04-06T13:31:54.259511Z","shell.execute_reply.started":"2025-04-06T13:31:36.236765Z","shell.execute_reply":"2025-04-06T13:31:54.258870Z"},"id":"f281acdf","papermill":{"duration":14.035528,"end_time":"2025-03-21T01:39:17.502887","exception":false,"start_time":"2025-03-21T01:39:03.467359","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"maximnaddaf/ct-multi-modal-dataset\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"id":"iQn-gGLSQNts","outputId":"04e3e348-87e5-41aa-bbb9-ed2e7ea4f5a0","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:31:54.260474Z","iopub.execute_input":"2025-04-06T13:31:54.261033Z","iopub.status.idle":"2025-04-06T13:31:54.482200Z","shell.execute_reply.started":"2025-04-06T13:31:54.261009Z","shell.execute_reply":"2025-04-06T13:31:54.481420Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def count_directories(path):\n    # List all items in the directory and filter for directories\n    return len([d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))])\n\n\nnon_healthy = \"/kaggle/input/Processed_CT/CT-23\"\nhealthy = \"/kaggle/input/Processed_CT/CT-0\"\n\nprint(f\"Number of directories in non healthy: {count_directories(non_healthy)}\")\nprint(f\"Number of directories in healthy: {count_directories(healthy)}\")","metadata":{"id":"flmWMeWaPXaA","outputId":"e29008d0-6659-4a02-afd4-1addc425dbf5","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:31:54.483800Z","iopub.execute_input":"2025-04-06T13:31:54.484005Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"Number of directories in non healthy: 172\nNumber of directories in healthy: 172\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Function to load a .nii.gz file\ndef load_nii(file_path):\n    return nib.load(file_path).get_fdata()\n\n\ndef rotate(volume):\n    # define some rotation angles\n    angles = [-20, -10, -5, 5, 10, 20]\n    # pick angles at random\n    angle = random.choice(angles)\n    # rotate volume\n    volume = ndimage.rotate(volume, angle, reshape=False)\n    volume[volume < 0] = 0\n    volume[volume > 1] = 1\n    return volume\n\n    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n    return augmented_volume\n\n\n\nclass MultiModalDataset(Sequence):\n    def __init__(self, patient_dirs, labels, batch_size, fold, shuffle=True):\n        self.patient_dirs = patient_dirs\n        self.labels = labels\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.fold = fold\n        self.indices = np.arange(len(self.patient_dirs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.floor(len(self.patient_dirs) / self.batch_size))\n\n    def __getitem__(self, index):\n        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n        batch_patient_dirs = [self.patient_dirs[i] for i in batch_indices]\n        batch_labels = [self.labels[i] for i in batch_indices]\n\n        # Load modalities\n        X = self.__load_modalities(batch_patient_dirs)\n        y = np.array(batch_labels)\n\n        X = tuple(tf.convert_to_tensor(x, dtype=tf.float32) for x in X)\n\n        return X, y\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n\n    def __load_modalities(self, patient_dirs):\n        modalities = {'split_1': [], 'split_2': []}\n\n        for patient_dir in patient_dirs:\n            patient_name = os.path.basename(patient_dir)\n\n            split_1_path = os.path.join(patient_dir, 'split_part_1.nii')\n            split_2_path = os.path.join(patient_dir, 'split_part_2.nii')\n\n            # Load each modality\n            split_1 = load_nii(split_1_path)\n            split_2 = load_nii(split_2_path)\n\n            if (self.fold == \"train\"):\n                split_1_augmented = rotate(split_1)\n                split_2_augmented = rotate(split_2)\n\n                modalities['split_1'].append(split_1_augmented)\n                modalities['split_2'].append(split_2_augmented)\n\n            elif (self.fold == \"test\"):\n                modalities['split_1'].append(split_1)\n                modalities['split_2'].append(split_2)\n\n\n        # Stack the modalities for each patient along the first axis (batch axis)\n        X = [\n            np.array(modalities['split_1']),\n            np.array(modalities['split_2']),\n        ]\n\n        return X\n\n# Define the paths to your directories\nhealthy_dir = healthy\nnon_healthy_dir = non_healthy\n\n# List all patient directories (assuming each patient has a folder in each group)\nhealthy_patients = [os.path.join(healthy_dir, f) for f in os.listdir(healthy_dir) if os.path.isdir(os.path.join(healthy_dir, f))]\nnon_healthy_patients = [os.path.join(non_healthy_dir, f) for f in os.listdir(non_healthy_dir) if os.path.isdir(os.path.join(non_healthy_dir, f))]\n\n# Initialize lists to hold the data and labels\npatient_dirs = []  # Will hold paths to the patient directories\nlabels = []  # Will hold the labels (0 for healthy, 1 for non-healthy)\n\n# Load the data from healthy patients\nfor patient in healthy_patients:\n    patient_dirs.append(patient)\n    labels.append(0)  # Label 0 for healthy\n\n# Load the data from non-healthy patients\nfor patient in non_healthy_patients:\n    patient_dirs.append(patient)\n    labels.append(1)  # Label 1 for non-healthy\n\n# Convert to NumPy arrays\npatient_dirs = np.array(patient_dirs)\nlabels = np.array(labels)\n\n# Shuffle the data\nindices = np.arange(len(patient_dirs))\nnp.random.shuffle(indices)\n\npatient_dirs = patient_dirs[indices]\nlabels = labels[indices]\n\n# Split the data into training and testing sets (80/20)\npatient_dirs_train, patient_dirs_test, labels_train, labels_test = train_test_split(patient_dirs, labels, test_size=0.2, random_state=42)\n\n# Create the training and testing dataset objects\ntrain_dataset = MultiModalDataset(patient_dirs_train, labels_train, batch_size=2, fold=\"train\", shuffle=True)\ntest_dataset = MultiModalDataset(patient_dirs_test, labels_test, batch_size=2, fold=\"test\", shuffle=False)\n\n# Check the length of the datasets\nprint(f\"Train Dataset Length: {len(train_dataset)}\")\nprint(f\"Test Dataset Length: {len(test_dataset)}\")\nprint(f\"Labels Dataset Length: {len(labels)}\")\n\nprint(\"\\n\")\n\nX, y = train_dataset[0]\n\nfor modality, data in zip(['split_1', 'split_2'], X):\n    print(f\"Shape of {modality} modality: {data.shape}\")\n\nprint(f\"label: {y}\")\n","metadata":{"trusted":true,"id":"6FNOjtQsQJVE","outputId":"6d9fce6d-7ff8-4188-d87e-63e0d8d39eef","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_scans_from_dataset(dataset):\n\n    fig = plt.figure(figsize=(20,8))\n\n    for i in range(1):\n        scans, label = dataset[i]\n        print(\"Number of scans:\", len(scans))\n        print(\"Dimension of CT scan:\", scans[0][0].shape)\n        print(\"label=\",label[i])\n        print(\"\\n\")\n\n        ax1 = plt.subplot(1,4,1)\n        ax1.imshow(scans[0][0][:,:, scans[0][0].shape[2] // 2], cmap=\"gray\")\n        ax1.set_title(\"Split_1, Middle Slice\")\n        ax2 = plt.subplot(1,4,2)\n        ax2.imshow(scans[1][0][:,:, scans[1][0].shape[2] // 2], cmap=\"gray\")\n        ax2.set_title(\"Split_2, Middle Slice\")\n\nplot_scans_from_dataset(train_dataset)\nplot_scans_from_dataset(test_dataset)","metadata":{"trusted":true,"id":"yfUo9tqVQJVF","outputId":"b94132b7-422f-458e-9f25-4a07190008b0","colab":{"base_uri":"https://localhost:8080/","height":997}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model(inputs):\n    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n\n    x = Conv3D(filters=16, kernel_size=3, activation=\"relu\", padding=\"same\")(inputs)\n    x = MaxPool3D(pool_size=2)(x)\n    x = BatchNormalization()(x)\n    \n    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n    x = MaxPool3D(pool_size=2)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.1)(x)\n    \n    # x = Conv3D(filters=32, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n    # x = MaxPool3D(pool_size=2)(x)\n    # x = BatchNormalization()(x)\n    # x = Dropout(0.1)(x)\n    \n    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n    x = MaxPool3D(pool_size=2)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n\n\n    return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.random.set_seed(42)\n\nsplit_1_input = Input(shape=(224, 224, 25, 1))\nsplit_1_model = get_model(split_1_input)\n\nsplit_2_input = Input(shape=(224, 224, 25, 1))\nsplit_2_model = get_model(split_2_input)\n\n\n# Concat models and add final block\nx = concatenate([split_1_model, split_2_model])\n\nx = GlobalAveragePooling3D()(x)\nx = Dense(units=700, activation=\"relu\")(x)\nx = Dropout(0.3)(x)\n\noutput_layer = Dense(units=1, activation=\"sigmoid\")(x)\n\n# Final model\ncnn_model = Model(inputs=[split_1_input, split_2_input], outputs=[output_layer], name=\"multi3dcnn\")\n\n# f1score = tf.keras.metrics.F1Score(average=\"weighted\",threshold=0.5)\n\ninitial_learning_rate = 0.0001\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=False\n)\n\nearly_stop = EarlyStopping(monitor = 'val_loss',\n                           patience = 20, mode = 'min', verbose = 1,\n                           restore_best_weights = True)\n\nmodel_save = ModelCheckpoint(f'3d_image_classification.keras',\n                             save_best_only = True,\n                             monitor = 'val_loss',\n                             mode = 'min', verbose = 1)\n\n# Compile final model\ncnn_model.compile(loss='binary_crossentropy',\n                  optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n                  metrics=[\"accuracy\", \"AUC\"])","metadata":{"trusted":true,"id":"HXmerBEHQJVG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_model(cnn_model, show_shapes=True, show_layer_names=True)","metadata":{"id":"4pB7bcrwT66-","outputId":"0628bbe1-e179-4ed2-ec4a-2d7cc785961c","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":1000}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cnn_model.summary()","metadata":{"trusted":true,"id":"QJQe4j37QJVG","outputId":"06db6e94-dd7d-4636-a517-826e34ba3d1c","colab":{"base_uri":"https://localhost:8080/","height":1000}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = cnn_model.fit(train_dataset, validation_data=test_dataset, epochs=50, callbacks=[model_save, early_stop])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cnn_model.evaluate(test_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cnn_model.save('/kaggle/working/3d_image_classification.keras')\ncnn_model_improved = load_model('/kaggle/working/3d_image_classification.keras')","metadata":{"trusted":true,"id":"LOJ_6bEXJEFf"},"outputs":[],"execution_count":null}]}